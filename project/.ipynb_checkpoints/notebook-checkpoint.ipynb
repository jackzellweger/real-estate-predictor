{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be09d55",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9805eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# OPERATING SYSTEM STUFF\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "# BASIC DATA SCIENCE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# MACHINE LEARNING\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# MODEL PACKAGING\n",
    "import joblib\n",
    "\n",
    "# API STUFF\n",
    "import xlrd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# SQL\n",
    "import time\n",
    "from sqlalchemy import create_engine, text, String, Integer, Float, Boolean, MetaData, Table, select\n",
    "from sqlalchemy.exc import ProgrammingError # ProgrammingError catches SQL write exceptions\n",
    "from sqlalchemy.sql import and_\n",
    "\n",
    "# GEOCODING\n",
    "from geopy.geocoders import GoogleV3\n",
    "\n",
    "# CONFIGURATION FILES\n",
    "import config\n",
    "importlib.reload(config)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "# OTHER\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f3707",
   "metadata": {},
   "source": [
    "## Create databases / database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fa97a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence errors\n",
    "os.environ['SQLALCHEMY_WARN_20'] = '0'\n",
    "os.environ['SQLALCHEMY_SILENCE_UBER_WARNING'] = '1'\n",
    "\n",
    "# Database params & credentials\n",
    "username = config.DB_USERNAME\n",
    "password = config.DB_PASSWORD\n",
    "hostname = config.DB_HOSTNAME\n",
    "database_name = config.DB_NAME\n",
    "\n",
    "# Table names\n",
    "geocodes_sql_table_name = 'geocodes'\n",
    "sales_sql_table_name = 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "573ae528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established successfully.\n",
      "SQLAlchemy warnings silenced.\n",
      "Table 'geocodes' already exists. Not resetting!\n",
      "Column PRIMARY_KEY already exists in table geocodes.\n",
      "PRIMARY_KEY column values set in table geocodes.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to establish a connection to the database\n",
    "engine = helpers.connect_to_database(username, password, hostname)\n",
    "\n",
    "if engine is not None:\n",
    "    # See file `helpers.py` for function documentation \n",
    "    engine = helpers.create_database(engine, database_name)\n",
    "    helpers.silence_warnings()\n",
    "    helpers.create_table_from_csv(engine, 'geocodes', 'geocodes_export_backup_1.csv')\n",
    "    helpers.add_primary_key(engine, 'geocodes', 'PRIMARY_KEY')\n",
    "    helpers.set_primary_key(engine, 'geocodes', 'PRIMARY_KEY', \"`BOROUGH`, '_', `ADDRESS`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bb892",
   "metadata": {},
   "source": [
    "## Download new sales data from NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a81f85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array that will hold our NYC Housing DataFrames\n",
    "data = []\n",
    "\n",
    "# Pull data from the NYC website\n",
    "for url in helpers.dataURLs:\n",
    "    # Read Excel file and skip the first 4 rows\n",
    "    df = pd.read_excel(url, skiprows=4, engine=\"openpyxl\")\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12066813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes from the nyc housing website\n",
    "combined = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Rename the 'BOROUGH' column to 'BOROUGH CODE'\n",
    "combined = combined.rename(columns={'BOROUGH': 'BOROUGH CODE'})\n",
    "\n",
    "# Define the mapping for borough codes to borough names\n",
    "borough_mapping = {1: 'MANHATTAN', 2: 'BRONX', 3: 'BROOKLYN', 4: 'QUEENS', 5: 'STATEN ISLAND'}\n",
    "\n",
    "# Create a new 'BOROUGH' column based on 'BOROUGH CODE'\n",
    "borough = combined['BOROUGH CODE'].map(borough_mapping)\n",
    "\n",
    "# Insert the new 'BOROUGH' column into the DataFrame right after the 'BOROUGH CODE' column\n",
    "combined.insert(loc=1, column='BOROUGH', value=borough)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1642259",
   "metadata": {},
   "source": [
    "## Filter new sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "541b025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that contain the string 'N/A' anywhere in the address column...\n",
    "combined = combined[~combined['ADDRESS'].str.contains('N/A')]\n",
    "\n",
    "# Define thresholds for \"close to zero\"\n",
    "thresholds = {\n",
    "    'SALE PRICE': 100000,\n",
    "    'GROSS SQUARE FEET': 100,\n",
    "    'LAND SQUARE FEET': 100\n",
    "}\n",
    "\n",
    "#Remove rows with values \"close to zero\"\n",
    "data_clean = combined.copy()\n",
    "for col, threshold in thresholds.items():\n",
    "    data_clean = data_clean[data_clean[col] >= threshold]\n",
    "\n",
    "# List of columns to remove outliers from\n",
    "cols_to_check = list(thresholds.keys())\n",
    "\n",
    "# Remove outliers\n",
    "for col in cols_to_check:\n",
    "    # Calculate the IQR of each column\n",
    "    Q1 = data_clean[col].quantile(0.25)\n",
    "    Q3 = data_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers\n",
    "    data_clean = data_clean[(data_clean[col] >= lower_bound) & (data_clean[col] <= upper_bound)]\n",
    "\n",
    "# Reset `combined` to the clean data\n",
    "combined = data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1768985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create histograms for each column\\nfig, axs = plt.subplots(1, len(cols_to_check), figsize=(15, 5))\\nx\\n# Create histograms for each column\\nfor i, col in enumerate(cols_to_check):\\n    axs[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black')\\n    axs[i].set_title(f'{col}')\\n\\n# Tight layout\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot new distributions for sanity check\n",
    "'''\n",
    "# Create histograms for each column\n",
    "fig, axs = plt.subplots(1, len(cols_to_check), figsize=(15, 5))\n",
    "x\n",
    "# Create histograms for each column\n",
    "for i, col in enumerate(cols_to_check):\n",
    "    axs[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black')\n",
    "    axs[i].set_title(f'{col}')\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "354596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the contents of `combined` to the `sales` SQL table...\n",
    "with engine.connect() as connection:\n",
    "    combined.to_sql(sales_sql_table_name, con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bbbab",
   "metadata": {},
   "source": [
    "# Update geocodes table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d145a",
   "metadata": {},
   "source": [
    "### Set up tables for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c868334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of just geo-columns from NYC data\n",
    "geocodes_local = combined[['BOROUGH CODE', 'BOROUGH', 'NEIGHBORHOOD', 'ADDRESS']].copy()\n",
    "geocodes_local['LATITUDE'], geocodes_local['LONGITUDE'], geocodes_local['GEOCODING ERR'] = None, None, False\n",
    "geocodes_local['PRIMARY_KEY'] = geocodes_local['BOROUGH'] + '_' + geocodes_local['ADDRESS']\n",
    "\n",
    "# Load geocodes SQL table into a DataFrame\n",
    "geocodes_table_response = pd.read_sql_query(f\"SELECT * FROM {geocodes_sql_table_name}\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c290d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows in NYC data not in our existing geocoding data\n",
    "missing_rows = geocodes_local[~geocodes_local['PRIMARY_KEY'].isin(geocodes_table_response['PRIMARY_KEY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "060502af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d967d1e7af418d9baa4eb63120b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Geocode the rows missing from the SQL table\n",
    "tqdm.pandas()\n",
    "missing_rows = missing_rows.progress_apply(helpers.geolocate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c015421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index on the dataframe so that we ensure we don't have duplicates\n",
    "missing_rows.drop_duplicates(subset='PRIMARY_KEY', keep='first', inplace=True)\n",
    "missing_rows.set_index('PRIMARY_KEY', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ef86175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing rows back to the SQL table with the geocodes\n",
    "with engine.connect() as connection:\n",
    "    missing_rows.to_sql(geocodes_sql_table_name, con=engine, if_exists='append', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e297a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if the append worked. If `missing_rows` is empty, it did.\n",
    "with engine.connect() as connection:\n",
    "    geocodes_table_response = pd.read_sql_query(f\"SELECT * FROM {geocodes_sql_table_name}\", engine)\n",
    "\n",
    "missing_rows = geocodes_local[~geocodes_local['PRIMARY_KEY'].isin(geocodes_table_response['PRIMARY_KEY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d29c76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames on 'BOROUGH' and 'ADDRESS'\n",
    "combined = combined.merge(geocodes_table_response[['BOROUGH', 'ADDRESS', 'LATITUDE', 'LONGITUDE']], \n",
    "                          on=['BOROUGH', 'ADDRESS'], \n",
    "                          how='left', \n",
    "                          suffixes=('', '_y'))\n",
    "\n",
    "# The merge could result in duplicate 'LATITUDE' and 'LONGITUDE'\n",
    "# columns if they exist in the `combined` dataframe.\n",
    "# We'll handle this by dropping the duplicate columns.\n",
    "\n",
    "# List of duplicate columns\n",
    "duplicate_columns = ['LATITUDE_y', 'LONGITUDE_y']\n",
    "\n",
    "# Drop duplicate columns from `combined`\n",
    "combined = combined.drop(columns=duplicate_columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5360857",
   "metadata": {},
   "source": [
    "## Build mapping between NYC and Zillow housing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cc083c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: some categories were not be mapped, those rows were dropped.\n"
     ]
    }
   ],
   "source": [
    "# First, create the inverted mapping dictionary\n",
    "# invert_mapping = {building_class: zillow_cat for zillow_cat, building_class_list in helpers.category_mapping.items() for building_class in building_class_list}\n",
    "\n",
    "category_mapping = helpers.category_mapping\n",
    "\n",
    "# Then, use the map function to create the new column\n",
    "combined['GROUPED CATEGORY'] = combined['BUILDING CLASS CATEGORY'].map(category_mapping)\n",
    "\n",
    "# Check if there are any missing values in the new column (i.e., categories that couldn't be mapped)\n",
    "if combined['GROUPED CATEGORY'].isna().any():\n",
    "    combined = combined.dropna(subset=['GROUPED CATEGORY'])\n",
    "    print(\"Warning: some categories were not be mapped, those rows were dropped.\")\n",
    "\n",
    "combined.to_csv('for_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dcb4a",
   "metadata": {},
   "source": [
    "## Choosing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1672322e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH CODE           0\n",
       "GROSS SQUARE FEET      0\n",
       "LAND SQUARE FEET       0\n",
       "GROUPED CATEGORY       0\n",
       "LATITUDE             814\n",
       "LONGITUDE            814\n",
       "SALE PRICE             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the features we are interested in\n",
    "selected_features = ['BOROUGH CODE', #'ZIP CODE',\n",
    "                     'GROSS SQUARE FEET', 'LAND SQUARE FEET', 'GROUPED CATEGORY', \n",
    "                     'LATITUDE', 'LONGITUDE', 'SALE PRICE']\n",
    "\n",
    "# Create a new DataFrame with only these features\n",
    "df = combined[selected_features]\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f006c294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH CODE         0\n",
       "GROSS SQUARE FEET    0\n",
       "LAND SQUARE FEET     0\n",
       "GROUPED CATEGORY     0\n",
       "LATITUDE             0\n",
       "LONGITUDE            0\n",
       "SALE PRICE           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing latitude or longitude\n",
    "df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Check again for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75ecf5",
   "metadata": {},
   "source": [
    "## Encode using a `scikit-learn` encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7df16fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be scaled and one-hot encoded\n",
    "cols_to_encode = ['BOROUGH CODE','GROUPED CATEGORY']\n",
    "\n",
    "cols_to_scale = ['GROSS SQUARE FEET',\n",
    "                 'LAND SQUARE FEET',\n",
    "                 'LATITUDE',\n",
    "                 'LONGITUDE',\n",
    "                 'SALE PRICE']\n",
    "\n",
    "# Initialize the transformers\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', scaler, cols_to_scale),\n",
    "        ('ohe', ohe, cols_to_encode)])\n",
    "\n",
    "# Apply the transformations\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "ohe_feature_names = list(preprocessor.named_transformers_['ohe'].get_feature_names(input_features=cols_to_encode))\n",
    "\n",
    "# Combine the feature names\n",
    "feature_names = cols_to_scale + ohe_feature_names\n",
    "\n",
    "# Convert the array back into a DataFrame\n",
    "df_processed = pd.DataFrame(df_processed, columns=feature_names)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "#df_processed.head()\n",
    "\n",
    "df_encoded = df_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b66461",
   "metadata": {},
   "source": [
    "## Split the data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35226f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15792, 13), (3949, 13))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into features and target\n",
    "X = df_encoded.drop('SALE PRICE', axis=1)\n",
    "y = df_encoded['SALE PRICE']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24ed3f",
   "metadata": {},
   "source": [
    "## Define random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be063865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35992026281957035, 0.4265471961128181)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the MAE\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the test set and calculate the MAE\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "mae_train, mae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18567166",
   "metadata": {},
   "source": [
    "## Package up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd2dd27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/model.joblib']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump the model to a shared docker volume...\n",
    "joblib.dump(model, 'model/model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8256686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/preprocessor.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "joblib.dump(model, 'model.joblib')\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, './model/model.joblib')\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, './model/preprocessor.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
