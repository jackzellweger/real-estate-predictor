{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be09d55",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af9805eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# OPERATING SYSTEM STUFF\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "# BASIC STUFF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# MACHINE LEARNING\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# MODEL PACKAGING\n",
    "import joblib\n",
    "\n",
    "# API STUFF\n",
    "import xlrd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# SQL STUFF\n",
    "import time\n",
    "from sqlalchemy import create_engine, text, String, Integer, Float, Boolean, MetaData, Table, select\n",
    "from sqlalchemy.exc import ProgrammingError # ProgrammingError catches SQL write exceptions\n",
    "from sqlalchemy.sql import and_\n",
    "\n",
    "# GEOCODING STUFF\n",
    "from geopy.geocoders import GoogleV3\n",
    "\n",
    "# CONFIGURATION STUFF\n",
    "import config\n",
    "importlib.reload(config)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "# OTHER STUFF\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f3707",
   "metadata": {},
   "source": [
    "## Create databases / database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fa97a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence errors\n",
    "os.environ['SQLALCHEMY_WARN_20'] = '0'\n",
    "os.environ['SQLALCHEMY_SILENCE_UBER_WARNING'] = '1'\n",
    "\n",
    "# Database params & credentials\n",
    "username = config.DB_USERNAME\n",
    "password = config.DB_PASSWORD\n",
    "hostname = config.DB_HOSTNAME\n",
    "database_name = config.DB_NAME\n",
    "\n",
    "# Table names\n",
    "geocodes_sql_table_name = 'geocodes'\n",
    "sales_sql_table_name = 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "573ae528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established successfully.\n",
      "SQLAlchemy warnings silenced.\n",
      "Table geocodes already exists\n",
      "Table 'geocodes' created from csv 'geocodes_export_backup.csv' successfully, or already exists.\n",
      "Column PRIMARY_KEY already exists in table geocodes.\n",
      "PRIMARY_KEY column values set in table geocodes.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to establish a connection to the database\n",
    "engine = helpers.connect_to_database(username, password, hostname)\n",
    "\n",
    "if engine is not None:\n",
    "    # See file `helpers.py` for function documentation \n",
    "    engine = helpers.create_database(engine, database_name)\n",
    "    helpers.silence_warnings()\n",
    "    helpers.create_table_from_csv(\n",
    "        engine, 'geocodes', 'geocodes_export_backup.csv')\n",
    "    helpers.add_primary_key(\n",
    "        engine, 'geocodes', 'PRIMARY_KEY')\n",
    "    helpers.set_primary_key(\n",
    "        engine, 'geocodes', 'PRIMARY_KEY', \"`BOROUGH`, '_', `ADDRESS`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bb892",
   "metadata": {},
   "source": [
    "## Download new sales data from NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a81f85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array that will hold our NYC Housing DataFrames\n",
    "data = []\n",
    "\n",
    "# Pull data from the NYC website\n",
    "for url in helpers.dataURLs:\n",
    "    # Read Excel file and skip the first 4 rows\n",
    "    df = pd.read_excel(url, skiprows=4, engine=\"openpyxl\")\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12066813",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = helpers.combineHousingDataSets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc4c5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'BOROUGH' column to 'BOROUGH CODE'\n",
    "combined = combined.rename(columns={\"BOROUGH\": \"BOROUGH CODE\"})\n",
    "\n",
    "# Define the mapping for borough codes to borough names\n",
    "borough_mapping = {\n",
    "    1: \"MANHATTAN\",\n",
    "    2: \"BRONX\",\n",
    "    3: \"BROOKLYN\",\n",
    "    4: \"QUEENS\",\n",
    "    5: \"STATEN ISLAND\",\n",
    "}\n",
    "\n",
    "# Create a new 'BOROUGH' column based on 'BOROUGH CODE'\n",
    "borough = combined[\"BOROUGH CODE\"].map(borough_mapping)\n",
    "\n",
    "# Insert the new 'BOROUGH' column into the DataFrame right after the 'BOROUGH CODE' column\n",
    "combined.insert(loc=1, column=\"BOROUGH\", value=borough)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1642259",
   "metadata": {},
   "source": [
    "## Filter new sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cff3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that contain the string 'N/A' anywhere in the address column...\n",
    "combined = combined[~combined['ADDRESS'].str.contains('N/A')]\n",
    "\n",
    "# Define thresholds for \"close to zero\"\n",
    "thresholds = {\n",
    "    'SALE PRICE': 100000,\n",
    "    'GROSS SQUARE FEET': 100,\n",
    "    'LAND SQUARE FEET': 100\n",
    "}\n",
    "\n",
    "# Filter outliers\n",
    "combined = helpers.filterOutliers(combined, thresholds, 0.15, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1768985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create histograms for each column\\nfig, axs = plt.subplots(1, len(cols_to_check), figsize=(15, 5))\\nx\\n# Create histograms for each column\\nfor i, col in enumerate(cols_to_check):\\n    axs[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black')\\n    axs[i].set_title(f'{col}')\\n\\n# Tight layout\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot new distributions for sanity check\n",
    "\"\"\"\n",
    "# Create histograms for each column\n",
    "fig, axs = plt.subplots(1, len(cols_to_check), figsize=(15, 5))\n",
    "x\n",
    "# Create histograms for each column\n",
    "for i, col in enumerate(cols_to_check):\n",
    "    axs[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black')\n",
    "    axs[i].set_title(f'{col}')\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "354596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the contents of `combined` to the `sales` SQL table...\n",
    "# FIXME: Is this necessary? Can we do away with the sales table SQL stuff\n",
    "with engine.connect() as connection:\n",
    "    combined.to_sql(sales_sql_table_name, con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bbbab",
   "metadata": {},
   "source": [
    "# Update geocodes table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d145a",
   "metadata": {},
   "source": [
    "### Set up tables for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df49ee88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99694512bc54445c83b7394afc300ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    #missing_rows = helpers.check_missing_rows(combined, geocodes_sql_table_name, engine)\n",
    "    missing_rows = helpers.check_missing_rows(combined, geocodes_sql_table_name, engine).head(5)\n",
    "\n",
    "if missing_rows is not False:\n",
    "    tqdm.pandas()\n",
    "    try:\n",
    "        missing_rows = missing_rows.progress_apply(lambda x: helpers.geolocate(x, config.GOOGLE_API_KEY), axis=1)\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        print(\"We'll work with old data for now...\")\n",
    "        missing_rows = missing_rows.drop(missing_rows.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c015421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index on the dataframe ensuring we don't have duplicates\n",
    "missing_rows.drop_duplicates(subset='PRIMARY_KEY', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ef86175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing rows back to the SQL table with the geocodes\n",
    "with engine.connect() as connection:\n",
    "    missing_rows.to_sql(geocodes_sql_table_name, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e20cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if the append worked.\n",
    "# If ValueError is not raised, then the append did not work.\n",
    "with engine.connect() as connection:\n",
    "    # Resets the index\n",
    "    missing_rows.reset_index(drop=False, inplace=True)\n",
    "    if not helpers.is_local_sql_subset(connection, missing_rows, geocodes_sql_table_name):\n",
    "        raise ValueError(\n",
    "            \"Error appending local geocode data to SQL table.\\\n",
    "            Local geocode table not a subset of SQL geocode table.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "676fbeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull geocodes back down from SQL table\n",
    "with engine.connect() as connection:\n",
    "        geocodes_table_response = pd.read_sql_query(\n",
    "            f\"SELECT * FROM {geocodes_sql_table_name}\", engine\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f103b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create primary key and merge geocodes on it\n",
    "combined['PRIMARY_KEY'] = combined['BOROUGH'].astype(str) + \"_\" + combined['ADDRESS'].astype(str)\n",
    "combined = combined.merge(geocodes_table_response[['PRIMARY_KEY', 'LATITUDE', 'LONGITUDE']], \n",
    "                          on='PRIMARY_KEY', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5360857",
   "metadata": {},
   "source": [
    "## Build mapping between NYC and Zillow housing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cc083c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: some categories were not be mapped, those rows were dropped.\n"
     ]
    }
   ],
   "source": [
    "# First, create the inverted mapping dictionary\n",
    "# invert_mapping = {building_class: zillow_cat for zillow_cat, building_class_list in helpers.category_mapping.items() for building_class in building_class_list}\n",
    "\n",
    "category_mapping = helpers.category_mapping\n",
    "\n",
    "# Then, use the map function to create the new column\n",
    "combined['GROUPED CATEGORY'] = combined['BUILDING CLASS CATEGORY'].map(category_mapping)\n",
    "\n",
    "# Check if there are any missing values in the new column (i.e., categories that couldn't be mapped)\n",
    "if combined['GROUPED CATEGORY'].isna().any():\n",
    "    combined = combined.dropna(subset=['GROUPED CATEGORY'])\n",
    "    print(\"Warning: some categories were not be mapped, those rows were dropped.\")\n",
    "\n",
    "combined.to_csv('for_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dcb4a",
   "metadata": {},
   "source": [
    "## Choosing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1672322e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select the features we are interested in\n",
    "selected_features = ['BOROUGH CODE', #'ZIP CODE',\n",
    "                     'GROSS SQUARE FEET', 'LAND SQUARE FEET', 'GROUPED CATEGORY', \n",
    "                     'LATITUDE', 'LONGITUDE', 'SALE PRICE']\n",
    "\n",
    "# Create a new DataFrame with only these features\n",
    "df = combined[selected_features]\n",
    "\n",
    "# Check for missing values\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f006c294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing latitude or longitude\n",
    "df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Check again for missing values\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75ecf5",
   "metadata": {},
   "source": [
    "## Encode using a `scikit-learn` encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7df16fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be scaled and one-hot encoded\n",
    "cols_to_encode = ['BOROUGH CODE','GROUPED CATEGORY']\n",
    "\n",
    "cols_to_scale = ['GROSS SQUARE FEET',\n",
    "                 'LAND SQUARE FEET',\n",
    "                 'LATITUDE',\n",
    "                 'LONGITUDE',\n",
    "                 'SALE PRICE']\n",
    "\n",
    "# Initialize the transformers\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', scaler, cols_to_scale),\n",
    "        ('ohe', ohe, cols_to_encode)])\n",
    "\n",
    "# Apply the transformations\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "ohe_feature_names = list(preprocessor.named_transformers_['ohe'].get_feature_names(input_features=cols_to_encode))\n",
    "\n",
    "# Combine the feature names\n",
    "feature_names = cols_to_scale + ohe_feature_names\n",
    "\n",
    "# Convert the array back into a DataFrame\n",
    "df_processed = pd.DataFrame(df_processed, columns=feature_names)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "#df_processed.head()\n",
    "\n",
    "df_encoded = df_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b66461",
   "metadata": {},
   "source": [
    "## Split the data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35226f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df_encoded.drop('SALE PRICE', axis=1)\n",
    "y = df_encoded['SALE PRICE']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "# X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24ed3f",
   "metadata": {},
   "source": [
    "## Define random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be063865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the MAE\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the test set and calculate the MAE\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# mae_train, mae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18567166",
   "metadata": {},
   "source": [
    "## Package up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8256686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/preprocessor.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump the model to a shared docker volume...\n",
    "joblib.dump(model, 'model/model.joblib')\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'model.joblib')\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, './model/model.joblib')\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, './model/preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3d07b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['LATITUDE', 'LONGITUDE', 'SALE PRICE']].to_csv('listings_with_price.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab8469cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOROUGH CODE</th>\n",
       "      <th>GROSS SQUARE FEET</th>\n",
       "      <th>LAND SQUARE FEET</th>\n",
       "      <th>GROUPED CATEGORY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>SALE PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4400.000000</td>\n",
       "      <td>2116.000000</td>\n",
       "      <td>Single-family home</td>\n",
       "      <td>40.721665</td>\n",
       "      <td>-73.978312</td>\n",
       "      <td>399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2790.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>40.724210</td>\n",
       "      <td>-73.978491</td>\n",
       "      <td>2999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8625.000000</td>\n",
       "      <td>2204.000000</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>40.721688</td>\n",
       "      <td>-73.979215</td>\n",
       "      <td>16800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8625.000000</td>\n",
       "      <td>2204.000000</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>40.721631</td>\n",
       "      <td>-73.979227</td>\n",
       "      <td>16800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9750.000000</td>\n",
       "      <td>2302.000000</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>40.723224</td>\n",
       "      <td>-73.978226</td>\n",
       "      <td>158822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24405</th>\n",
       "      <td>5</td>\n",
       "      <td>1760.000000</td>\n",
       "      <td>2379.000000</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>40.537036</td>\n",
       "      <td>-74.218932</td>\n",
       "      <td>695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24406</th>\n",
       "      <td>5</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>3147.000000</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>40.536586</td>\n",
       "      <td>-74.222383</td>\n",
       "      <td>625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24407</th>\n",
       "      <td>5</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>3147.000000</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>40.536586</td>\n",
       "      <td>-74.222383</td>\n",
       "      <td>815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24408</th>\n",
       "      <td>5</td>\n",
       "      <td>1176.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>40.535577</td>\n",
       "      <td>-74.218552</td>\n",
       "      <td>975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24409</th>\n",
       "      <td>5</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>3728.000000</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>40.532316</td>\n",
       "      <td>-74.222781</td>\n",
       "      <td>790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20651 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BOROUGH CODE  GROSS SQUARE FEET  LAND SQUARE FEET    GROUPED CATEGORY  \\\n",
       "0                 1        4400.000000       2116.000000  Single-family home   \n",
       "1                 1        2790.000000       1503.000000              Duplex   \n",
       "2                 1        8625.000000       2204.000000           Apartment   \n",
       "3                 1        8625.000000       2204.000000           Apartment   \n",
       "4                 1        9750.000000       2302.000000           Apartment   \n",
       "...             ...                ...               ...                 ...   \n",
       "24405             5        1760.000000       2379.000000              Duplex   \n",
       "24406             5        2400.000000       3147.000000              Duplex   \n",
       "24407             5        2400.000000       3147.000000              Duplex   \n",
       "24408             5        1176.000000       4600.000000              Duplex   \n",
       "24409             5        1960.000000       3728.000000              Duplex   \n",
       "\n",
       "       LATITUDE  LONGITUDE  SALE PRICE  \n",
       "0     40.721665 -73.978312      399000  \n",
       "1     40.724210 -73.978491     2999999  \n",
       "2     40.721688 -73.979215    16800000  \n",
       "3     40.721631 -73.979227    16800000  \n",
       "4     40.723224 -73.978226      158822  \n",
       "...         ...        ...         ...  \n",
       "24405 40.537036 -74.218932      695000  \n",
       "24406 40.536586 -74.222383      625000  \n",
       "24407 40.536586 -74.222383      815000  \n",
       "24408 40.535577 -74.218552      975000  \n",
       "24409 40.532316 -74.222781      790000  \n",
       "\n",
       "[20651 rows x 7 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23172f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac35005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
