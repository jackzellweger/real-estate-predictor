{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d0db9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c257fe13b29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# GEOCODING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleV3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# CONFIGURATION FILES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "# HELPERS\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# OPERATING SYSTEM STUFF\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "# BASIC DATA SCIENCE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# MACHINE LEARNING\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# MODEL PACKAGING\n",
    "import joblib\n",
    "\n",
    "# API STUFF\n",
    "import xlrd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine, text, String, Integer, Float, Boolean, MetaData, Table, select\n",
    "from sqlalchemy.exc import ProgrammingError # ProgrammingError catches SQL write exceptions\n",
    "from sqlalchemy.sql import and_\n",
    "\n",
    "# GEOCODING\n",
    "from geopy.geocoders import GoogleV3\n",
    "\n",
    "# CONFIGURATION FILES\n",
    "import config\n",
    "importlib.reload(config)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "# OTHER\n",
    "from tqdm.notebook import tqdm\n",
    "# This is a test change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa97a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATABASE CONNECTIONS\n",
    "\n",
    "# Silence errors\n",
    "os.environ['SQLALCHEMY_WARN_20'] = '0'\n",
    "os.environ['SQLALCHEMY_SILENCE_UBER_WARNING'] = '1'\n",
    "\n",
    "# Database params & credentials\n",
    "username = config.DB_USERNAME\n",
    "password = config.DB_PASSWORD\n",
    "hostname = config.DB_HOSTNAME\n",
    "database_name = config.DB_NAME\n",
    "\n",
    "# Create database connection\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{hostname}')\n",
    "\n",
    "# Create database and tables\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(f'CREATE DATABASE {database_name};'))\n",
    "except ProgrammingError:\n",
    "    pass\n",
    "\n",
    "# Reset connection to connect to specific database\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{hostname}/{database_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATABASES\n",
    "\n",
    "# Silence errors\n",
    "os.environ['SQLALCHEMY_WARN_20'] = '0'\n",
    "os.environ['SQLALCHEMY_SILENCE_UBER_WARNING'] = '1'\n",
    "\n",
    "# -------------- CREATE `geocodes` TABLE ---------------\n",
    "geocodes_sql_table_name = 'geocodes'\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Uses the geocode table backup if the table doesn't yet exist...\n",
    "    geocodes_reset_df = pd.read_csv('geocodes_export_backup.csv')\n",
    "    geocodes_reset_df.to_sql(geocodes_sql_table_name,\n",
    "                             con=engine,\n",
    "                             index=False,\n",
    "                             if_exists='fail')\n",
    "    \n",
    "    try: # This will only work if there is not already a column called 'PRIMARY_KEY'\n",
    "        connection.execute( # Set primary key\n",
    "            text(\n",
    "                f'ALTER TABLE {geocodes_sql_table_name} ADD COLUMN PRIMARY_KEY VARCHAR(255)'\n",
    "            )\n",
    "        )\n",
    "        print(f\"Column PRIMARY_KEY created in table {geocodes_sql_table_name}, database '{database_name}'.\")\n",
    "    except:\n",
    "        print(f\"Column PRIMARY_KEY already exists in in table {geocodes_sql_table_name}, database '{database_name}'.\")\n",
    "    \n",
    "    try:\n",
    "        connection.execute( # Set the values of the primary keys\n",
    "            text(\n",
    "                f'UPDATE {geocodes_sql_table_name} SET PRIMARY_KEY = CONCAT(`BOROUGH`, \\'_\\', `ADDRESS`)'\n",
    "            )\n",
    "        )\n",
    "        print(f\"PRIMARY_KEY column values set in database '{database_name}'.\")\n",
    "    except:\n",
    "        print(f\"PRIMARY_KEY column values set error in database '{database_name}'.\") \n",
    "        \n",
    "# 2.) -------------- CREATE `cat_map` TABLE ---------------\n",
    "mapping_list = [(k, v) for k, vals in helpers.mapping.items() for v in vals]\n",
    "mapping_df = pd.DataFrame(\n",
    "    mapping_list,\n",
    "    columns=['ZILLOW CATEGORY', 'BUILDING CLASS CATEGORY']\n",
    ")\n",
    "with engine.connect() as connection:\n",
    "    # Put the map into a SQL table. Why? Not sure. Might need it later!\n",
    "    mapping_df.to_sql('cat_map', con=engine, index=False, if_exists='replace')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array that will hold our NYC Housing DataFrames\n",
    "data = []\n",
    "\n",
    "# Pull data from the NYC website\n",
    "for url in helpers.dataURLs:\n",
    "    # Read Excel file and skip the first 4 rows\n",
    "    df = pd.read_excel(url, skiprows=4, engine=\"openpyxl\")\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12066813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes from the nyc housing website\n",
    "combined = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Rename the 'BOROUGH' column to 'BOROUGH CODE'\n",
    "combined = combined.rename(columns={'BOROUGH': 'BOROUGH CODE'})\n",
    "\n",
    "# Define the mapping for borough codes to borough names\n",
    "borough_mapping = {1: 'MANHATTAN', 2: 'BRONX', 3: 'BROOKLYN', 4: 'QUEENS', 5: 'STATEN ISLAND'}\n",
    "\n",
    "# Create a new 'BOROUGH' column based on 'BOROUGH CODE'\n",
    "borough = combined['BOROUGH CODE'].map(borough_mapping)\n",
    "\n",
    "# Insert the new 'BOROUGH' column into the DataFrame right after the 'BOROUGH CODE' column\n",
    "combined.insert(loc=1, column='BOROUGH', value=borough)\n",
    "\n",
    "# Remove rows that contain the string 'N/A'\n",
    "combined = combined[~combined['ADDRESS'].str.contains('N/A')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the contents of `combined` to the `sales` SQL table...\n",
    "with engine.connect() as connection:\n",
    "    combined.to_sql('sales', con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25473ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE GEOCODING TABLE & COPY TO SQL TABLE\n",
    "# Template Pandas DataFrame that we use to build the SQL database\n",
    "# using the 'geocoding_data_types_df' data structure we defined above\n",
    "geocodes = pd.DataFrame(columns=helpers.geocoding_data_types_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of the geographic information from `combined`\n",
    "geocodingTable = combined[['BOROUGH CODE', 'BOROUGH', 'NEIGHBORHOOD', 'ADDRESS']].copy()\n",
    "geocodingTable['LATITUDE'] = None\n",
    "geocodingTable['LONGITUDE'] = None\n",
    "geocodingTable['GEOCODING ERR'] = False\n",
    "geocodingTable['PRIMARY_KEY'] = geocodingTable['BOROUGH'] + '_' + geocodingTable['ADDRESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20808a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `geocodes_sql_table_name` (`geocodes`) SQL table into a DataFrame\n",
    "geocodes_table_response = pd.read_sql_query(f\"SELECT * FROM {geocodes_sql_table_name}\", engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND rows in NYC not in our 'geocodes table\n",
    "missing_rows = geocodingTable[~geocodingTable['PRIMARY_KEY'].isin(geocodes_table_response['PRIMARY_KEY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060502af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocode the rows missing from the SQL table `geocodes`\n",
    "tqdm.pandas()\n",
    "missing_rows = missing_rows.progress_apply(helpers.geolocate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index on the dataframe so that we ensure we don't have duplicates..\n",
    "missing_rows.set_index('PRIMARY_KEY', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef86175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing rows back to the SQL table w/ the geocodes\n",
    "with engine.connect() as connection:\n",
    "    missing_rows.to_sql(geocodes_sql_table_name, con=engine, if_exists='append', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e297a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can test to see if our append worked\n",
    "with engine.connect() as connection:\n",
    "    geocodes_table_response = pd.read_sql_query(f\"SELECT * FROM {geocodes_sql_table_name}\", engine)\n",
    "\n",
    "missing_rows = geocodingTable[~geocodingTable['PRIMARY_KEY'].isin(geocodes_table_response['PRIMARY_KEY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes on 'BOROUGH' and 'ADDRESS'\n",
    "combined = combined.merge(geocodes_table_response[['BOROUGH', 'ADDRESS', 'LATITUDE', 'LONGITUDE']], \n",
    "                          on=['BOROUGH', 'ADDRESS'], \n",
    "                          how='left', \n",
    "                          suffixes=('', '_y'))\n",
    "\n",
    "# The merge could result in duplicate 'LATITUDE' and 'LONGITUDE' columns if they exist in the `combined` dataframe.\n",
    "# We'll handle this by dropping the duplicate columns.\n",
    "\n",
    "# list of duplicate columns\n",
    "duplicate_columns = ['LATITUDE_y', 'LONGITUDE_y']\n",
    "\n",
    "# drop duplicate columns from `combined`\n",
    "combined = combined.drop(columns=duplicate_columns, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539372ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for \"close to zero\"\n",
    "thresholds = {\n",
    "    'SALE PRICE': 100000,\n",
    "    'GROSS SQUARE FEET': 100,\n",
    "    'LAND SQUARE FEET': 100\n",
    "}\n",
    "\n",
    "#Remove rows with values \"close to zero\"\n",
    "data_clean = combined.copy()\n",
    "for col, threshold in thresholds.items():\n",
    "    data_clean = data_clean[data_clean[col] >= threshold]\n",
    "\n",
    "# List of columns to remove outliers from\n",
    "cols_to_check = list(thresholds.keys())\n",
    "\n",
    "# Remove outliers\n",
    "for col in cols_to_check:\n",
    "    # Calculate the IQR of the column\n",
    "    Q1 = data_clean[col].quantile(0.25)\n",
    "    Q3 = data_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers\n",
    "    data_clean = data_clean[(data_clean[col] >= lower_bound) & (data_clean[col] <= upper_bound)]\n",
    "\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for each column\n",
    "fig, axs = plt.subplots(1, len(cols_to_check), figsize=(15, 5))\n",
    "\n",
    "# Create histograms for each column\n",
    "for i, col in enumerate(cols_to_check):\n",
    "    axs[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black')\n",
    "    axs[i].set_title(f'{col}')\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- ADDING INTERMEDIARY BUILDING CATEGORIES -------------------\n",
    "# Reset\n",
    "# combined.to_csv('sales.csv', index=False)\n",
    "combined = data_clean\n",
    "\n",
    "# HELPERS\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "\n",
    "# First, create the inverted mapping dictionary, as before\n",
    "invert_mapping = {building_class: zillow_cat for zillow_cat, building_class_list in helpers.intermediary_mapping.items() for building_class in building_class_list}\n",
    "\n",
    "# Then, use the map function to create the new column\n",
    "combined['GROUPED CATEGORY'] = combined['BUILDING CLASS CATEGORY'].map(invert_mapping)\n",
    "\n",
    "# Check if there are any missing values in the new column (i.e., categories that couldn't be mapped)\n",
    "if combined['GROUPED CATEGORY'].isna().any():\n",
    "    combined = combined.dropna(subset=['GROUPED CATEGORY'])\n",
    "    print(\"Warning: some categories were not be mapped, those rows were dropped.\")\n",
    "\n",
    "combined.to_csv('for_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dcb4a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we are interested in\n",
    "selected_features = ['BOROUGH CODE', #'ZIP CODE',\n",
    "                     'GROSS SQUARE FEET', 'LAND SQUARE FEET', 'GROUPED CATEGORY', \n",
    "                     'LATITUDE', 'LONGITUDE', 'SALE PRICE']\n",
    "\n",
    "# Create a new DataFrame with only these features\n",
    "df = combined[selected_features]\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing latitude or longitude\n",
    "df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Check again for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75ecf5",
   "metadata": {},
   "source": [
    "## One-hot encoding using an encoder, not `get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df16fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be scaled and one-hot encoded\n",
    "cols_to_encode = ['BOROUGH CODE','GROUPED CATEGORY']\n",
    "\n",
    "cols_to_scale = ['GROSS SQUARE FEET',\n",
    "                 'LAND SQUARE FEET',\n",
    "                 'LATITUDE',\n",
    "                 'LONGITUDE',\n",
    "                 'SALE PRICE']\n",
    "\n",
    "# Initialize the transformers\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', scaler, cols_to_scale),\n",
    "        ('ohe', ohe, cols_to_encode)])\n",
    "\n",
    "# Apply the transformations\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "ohe_feature_names = list(preprocessor.named_transformers_['ohe'].get_feature_names(input_features=cols_to_encode))\n",
    "\n",
    "# Combine the feature names\n",
    "feature_names = cols_to_scale + ohe_feature_names\n",
    "\n",
    "# Convert the array back into a DataFrame\n",
    "df_processed = pd.DataFrame(df_processed, columns=feature_names)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "df_processed.head()\n",
    "\n",
    "df_encoded = df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35226f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df_encoded.drop('SALE PRICE', axis=1)\n",
    "y = df_encoded['SALE PRICE']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5053ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc24ed3f",
   "metadata": {},
   "source": [
    "# Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be063865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the MAE\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the test set and calculate the MAE\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "mae_train, mae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18567166",
   "metadata": {},
   "source": [
    "# Packaging up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the model to a shared docker volume...\n",
    "dump(model, 'model/model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8256686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model, 'model.joblib')\n",
    "# Save the preprocessor\n",
    "dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "# Save the model\n",
    "dump(model, './model/model.joblib')\n",
    "# Save the preprocessor\n",
    "dump(preprocessor, './model/preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660ca4b",
   "metadata": {},
   "source": [
    "# Dummy API Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15271c31",
   "metadata": {},
   "source": [
    "### This will only work on a detached kernel in real life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc86479",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'BOROUGH CODE': 1,\n",
    "    'GROUPED CATEGORY': 'Single-family home',\n",
    "    'GROSS SQUARE FEET': 1000,\n",
    "    'LAND SQUARE FEET': 500,\n",
    "    'LATITUDE': 40.7128,\n",
    "    'LONGITUDE': -74.0060,\n",
    "    'SALE PRICE': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317393f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"./model/model.joblib\")\n",
    "encoder = joblib.load(\"./model/preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25caf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_api_df = pd.DataFrame([data], columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = encoder.transform(dummy_api_df)\n",
    "encoded_features = np.delete(encoded_features, 4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b27856",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(encoded_features)\n",
    "encoder.transformers_[0][1].inverse_transform([0,0,0,0,prediction[0]])[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f76d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = pd.DataFrame(columns=['BOROUGH CODE', 'GROSS SQUARE FEET', 'LAND SQUARE FEET',\n",
    "       'GROUPED CATEGORY', 'LATITUDE', 'LONGITUDE', 'SALE PRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9364df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_api_df = pd.DataFrame([data], columns=df_cols.columns)\n",
    "encoded_features = encoder.transform(dummy_api_df)\n",
    "encoded_features = np.delete(encoded_features, 4, axis=1)\n",
    "prediction = model.predict(encoded_features)\n",
    "encoder.transformers_[0][1].inverse_transform([0,0,0,0,prediction[0]])[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09d1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30c8b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6446b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f9c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870f3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4507a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47989bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9563a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac5aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f46efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72334c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba263f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45def30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf3fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c50ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146582ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af229a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73aa422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79784145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b78c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb28cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070eb2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c28a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4bbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc38a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a30d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
