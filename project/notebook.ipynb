{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d0db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# OPERATING SYSTEM STUFF\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "# BASIC DATA SCIENCE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# MACHINE LEARNING\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# MODEL PACKAGING\n",
    "import joblib\n",
    "\n",
    "# API STUFF\n",
    "import xlrd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# SQL\n",
    "import time\n",
    "from sqlalchemy import create_engine, text, String, Integer, Float, Boolean, MetaData, Table, select\n",
    "from sqlalchemy.exc import ProgrammingError # ProgrammingError catches SQL write exceptions\n",
    "from sqlalchemy.sql import and_\n",
    "\n",
    "# GEOCODING\n",
    "from geopy.geocoders import GoogleV3\n",
    "\n",
    "# CONFIGURATION FILES\n",
    "import config\n",
    "importlib.reload(config)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "# OTHER\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa97a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATABASE CONNECTIONS\n",
    "\n",
    "# Silence errors\n",
    "os.environ['SQLALCHEMY_WARN_20'] = '0'\n",
    "os.environ['SQLALCHEMY_SILENCE_UBER_WARNING'] = '1'\n",
    "\n",
    "# Database params & credentials\n",
    "username = config.DB_USERNAME\n",
    "password = config.DB_PASSWORD\n",
    "hostname = config.DB_HOSTNAME\n",
    "database_name = config.DB_NAME\n",
    "\n",
    "# Table names\n",
    "geocodes_sql_table_name = 'geocodes'\n",
    "sales_sql_table_name = 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573ae528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established successfully.\n",
      "SQLAlchemy warnings silenced.\n",
      "Table 'geocodes' already exists. Not resetting!\n",
      "Column PRIMARY_KEY already exists in table geocodes.\n",
      "PRIMARY_KEY column values set in table geocodes.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to establish a connection to the database\n",
    "engine = helpers.connect_to_database(username, password, hostname)\n",
    "\n",
    "if engine is not None:\n",
    "    # See file `helpers.py` for function documentation \n",
    "    engine = helpers.create_database(engine, database_name)\n",
    "    helpers.silence_warnings()\n",
    "    helpers.create_table_from_csv(engine, 'geocodes', 'geocodes_export_backup.csv')\n",
    "    helpers.add_primary_key(engine, 'geocodes', 'PRIMARY_KEY')\n",
    "    helpers.set_primary_key(engine, 'geocodes', 'PRIMARY_KEY', \"`BOROUGH`, '_', `ADDRESS`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7fbeef",
   "metadata": {},
   "source": [
    "## Download new housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81f85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array that will hold our NYC Housing DataFrames\n",
    "data = []\n",
    "\n",
    "# Pull data from the NYC website\n",
    "for url in helpers.dataURLs:\n",
    "    # Read Excel file and skip the first 4 rows\n",
    "    df = pd.read_excel(url, skiprows=4, engine=\"openpyxl\")\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12066813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes from the nyc housing website\n",
    "combined = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Rename the 'BOROUGH' column to 'BOROUGH CODE'\n",
    "combined = combined.rename(columns={'BOROUGH': 'BOROUGH CODE'})\n",
    "\n",
    "# Define the mapping for borough codes to borough names\n",
    "borough_mapping = {1: 'MANHATTAN', 2: 'BRONX', 3: 'BROOKLYN', 4: 'QUEENS', 5: 'STATEN ISLAND'}\n",
    "\n",
    "# Create a new 'BOROUGH' column based on 'BOROUGH CODE'\n",
    "borough = combined['BOROUGH CODE'].map(borough_mapping)\n",
    "\n",
    "# Insert the new 'BOROUGH' column into the DataFrame right after the 'BOROUGH CODE' column\n",
    "combined.insert(loc=1, column='BOROUGH', value=borough)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301a65f",
   "metadata": {},
   "source": [
    "## Filter the sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2008842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that contain the string 'N/A' anywhere in the address column...\n",
    "combined = combined[~combined['ADDRESS'].str.contains('N/A')]\n",
    "\n",
    "# Define thresholds for \"close to zero\"\n",
    "thresholds = {\n",
    "    'SALE PRICE': 100000,\n",
    "    'GROSS SQUARE FEET': 100,\n",
    "    'LAND SQUARE FEET': 100\n",
    "}\n",
    "\n",
    "#Remove rows with values \"close to zero\"\n",
    "data_clean = combined.copy()\n",
    "for col, threshold in thresholds.items():\n",
    "    data_clean = data_clean[data_clean[col] >= threshold]\n",
    "\n",
    "# List of columns to remove outliers from\n",
    "cols_to_check = list(thresholds.keys())\n",
    "\n",
    "# Remove outliers\n",
    "for col in cols_to_check:\n",
    "    # Calculate the IQR of the column\n",
    "    Q1 = data_clean[col].quantile(0.25)\n",
    "    Q3 = data_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers\n",
    "    data_clean = data_clean[(data_clean[col] >= lower_bound) & (data_clean[col] <= upper_bound)]\n",
    "\n",
    "# Reset `combined` to the clean data\n",
    "combined = data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b49d6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('geocodes_export_backup_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the contents of `combined` to the `sales` SQL table...\n",
    "with engine.connect() as connection:\n",
    "    combined.to_sql(sales_sql_table_name, con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35575af1",
   "metadata": {},
   "source": [
    "# Create geocoding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c868334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template geocoding table\n",
    "# geocodes = pd.DataFrame(columns=helpers.geocoding_data_types_df) # Is this table necessary?\n",
    "\n",
    "# Create a table of the geographic information from `combined`\n",
    "geocodingTable = combined[['BOROUGH CODE', 'BOROUGH', 'NEIGHBORHOOD', 'ADDRESS']].copy()\n",
    "geocodingTable['LATITUDE'], geocodingTable['LONGITUDE'], geocodingTable['GEOCODING ERR'] = None, None, False\n",
    "geocodingTable['PRIMARY_KEY'] = geocodingTable['BOROUGH'] + '_' + geocodingTable['ADDRESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e20808a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geocodes SQL table into a DataFrame\n",
    "geocodes_table_response = pd.read_sql_query(f\"SELECT * FROM {geocodes_sql_table_name}\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c290d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows in NYC data not in our existing geocoding data\n",
    "missing_rows = geocodingTable[~geocodingTable['PRIMARY_KEY'].isin(geocodes_table_response['PRIMARY_KEY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060502af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocode the rows missing from the SQL table `geocodes`\n",
    "tqdm.pandas()\n",
    "missing_rows = missing_rows.progress_apply(helpers.geolocate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index on the dataframe so that we ensure we don't have duplicates..\n",
    "missing_rows.set_index('PRIMARY_KEY', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef86175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing rows back to the SQL table w/ the geocodes\n",
    "with engine.connect() as connection:\n",
    "    missing_rows.to_sql(geocodes_sql_table_name, con=engine, if_exists='append', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e297a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can test to see if our append worked\n",
    "with engine.connect() as connection:\n",
    "    geocodes_table_response = pd.read_sql_query(f\"SELECT * FROM {geocodes_sql_table_name}\", engine)\n",
    "\n",
    "missing_rows = geocodingTable[~geocodingTable['PRIMARY_KEY'].isin(geocodes_table_response['PRIMARY_KEY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes on 'BOROUGH' and 'ADDRESS'\n",
    "combined = combined.merge(geocodes_table_response[['BOROUGH', 'ADDRESS', 'LATITUDE', 'LONGITUDE']], \n",
    "                          on=['BOROUGH', 'ADDRESS'], \n",
    "                          how='left', \n",
    "                          suffixes=('', '_y'))\n",
    "\n",
    "# The merge could result in duplicate 'LATITUDE' and 'LONGITUDE' columns if they exist in the `combined` dataframe.\n",
    "# We'll handle this by dropping the duplicate columns.\n",
    "\n",
    "# list of duplicate columns\n",
    "duplicate_columns = ['LATITUDE_y', 'LONGITUDE_y']\n",
    "\n",
    "# drop duplicate columns from `combined`\n",
    "combined = combined.drop(columns=duplicate_columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539372ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define thresholds for \"close to zero\"\n",
    "thresholds = {\n",
    "    'SALE PRICE': 100000,\n",
    "    'GROSS SQUARE FEET': 100,\n",
    "    'LAND SQUARE FEET': 100\n",
    "}\n",
    "\n",
    "#Remove rows with values \"close to zero\"\n",
    "data_clean = combined.copy()\n",
    "for col, threshold in thresholds.items():\n",
    "    data_clean = data_clean[data_clean[col] >= threshold]\n",
    "\n",
    "# List of columns to remove outliers from\n",
    "cols_to_check = list(thresholds.keys())\n",
    "\n",
    "# Remove outliers\n",
    "for col in cols_to_check:\n",
    "    # Calculate the IQR of the column\n",
    "    Q1 = data_clean[col].quantile(0.25)\n",
    "    Q3 = data_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers\n",
    "    data_clean = data_clean[(data_clean[col] >= lower_bound) & (data_clean[col] <= upper_bound)]\n",
    "\n",
    "# Reset `combined` to the clean data\n",
    "combined = data_clean\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new distributions for sanity check\n",
    "'''\n",
    "# Create histograms for each column\n",
    "fig, axs = plt.subplots(1, len(cols_to_check), figsize=(15, 5))\n",
    "\n",
    "# Create histograms for each column\n",
    "for i, col in enumerate(cols_to_check):\n",
    "    axs[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black')\n",
    "    axs[i].set_title(f'{col}')\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping building category\n",
    "\n",
    "# First, create the inverted mapping dictionary\n",
    "# invert_mapping = {building_class: zillow_cat for zillow_cat, building_class_list in helpers.category_mapping.items() for building_class in building_class_list}\n",
    "\n",
    "category_mapping = helpers.category_mapping\n",
    "\n",
    "# Then, use the map function to create the new column\n",
    "combined['GROUPED CATEGORY'] = combined['BUILDING CLASS CATEGORY'].map(category_mapping)\n",
    "\n",
    "# Check if there are any missing values in the new column (i.e., categories that couldn't be mapped)\n",
    "if combined['GROUPED CATEGORY'].isna().any():\n",
    "    combined = combined.dropna(subset=['GROUPED CATEGORY'])\n",
    "    print(\"Warning: some categories were not be mapped, those rows were dropped.\")\n",
    "\n",
    "combined.to_csv('for_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dcb4a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we are interested in\n",
    "selected_features = ['BOROUGH CODE', #'ZIP CODE',\n",
    "                     'GROSS SQUARE FEET', 'LAND SQUARE FEET', 'GROUPED CATEGORY', \n",
    "                     'LATITUDE', 'LONGITUDE', 'SALE PRICE']\n",
    "\n",
    "# Create a new DataFrame with only these features\n",
    "df = combined[selected_features]\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f598433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing latitude or longitude\n",
    "df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Check again for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75ecf5",
   "metadata": {},
   "source": [
    "## One-hot encoding using an encoder, not `get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df16fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be scaled and one-hot encoded\n",
    "cols_to_encode = ['BOROUGH CODE','GROUPED CATEGORY']\n",
    "\n",
    "cols_to_scale = ['GROSS SQUARE FEET',\n",
    "                 'LAND SQUARE FEET',\n",
    "                 'LATITUDE',\n",
    "                 'LONGITUDE',\n",
    "                 'SALE PRICE']\n",
    "\n",
    "# Initialize the transformers\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', scaler, cols_to_scale),\n",
    "        ('ohe', ohe, cols_to_encode)])\n",
    "\n",
    "# Apply the transformations\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "ohe_feature_names = list(preprocessor.named_transformers_['ohe'].get_feature_names(input_features=cols_to_encode))\n",
    "\n",
    "# Combine the feature names\n",
    "feature_names = cols_to_scale + ohe_feature_names\n",
    "\n",
    "# Convert the array back into a DataFrame\n",
    "df_processed = pd.DataFrame(df_processed, columns=feature_names)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "df_processed.head()\n",
    "\n",
    "df_encoded = df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35226f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df_encoded.drop('SALE PRICE', axis=1)\n",
    "y = df_encoded['SALE PRICE']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24ed3f",
   "metadata": {},
   "source": [
    "# Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be063865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the MAE\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Make predictions on the test set and calculate the MAE\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "mae_train, mae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18567166",
   "metadata": {},
   "source": [
    "# Packaging up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the model to a shared docker volume...\n",
    "joblib.dump(model, 'model/model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8256686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(model, 'model.joblib')\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, './model/model.joblib')\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, './model/preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec5dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe864f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce58dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb680e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2eea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f26f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
